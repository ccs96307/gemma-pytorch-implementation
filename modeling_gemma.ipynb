{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Gemma2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2Config {\n",
       "  \"architectures\": [\n",
       "    \"Gemma2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attn_logit_softcapping\": 50.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"cache_implementation\": \"hybrid\",\n",
       "  \"eos_token_id\": [\n",
       "    1,\n",
       "    107\n",
       "  ],\n",
       "  \"final_logit_softcapping\": 30.0,\n",
       "  \"head_dim\": 256,\n",
       "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
       "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
       "  \"hidden_size\": 2304,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 9216,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"model_type\": \"gemma2\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 26,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"query_pre_attn_scalar\": 256,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.42.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256000\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name_or_path = \"./models/google--gemma-2-2b-it\"\n",
    "\n",
    "config = Gemma2Config.from_pretrained(pretrained_model_name_or_path)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma2SdpaAttention:\n",
    "    def __init__(self, config: Gemma2Config) -> None:\n",
    "        self.num_heads = config.hidden_size\n",
    "        self.head_dim = config.head_dim\n",
    "\n",
    "        # GQA\n",
    "        self.q_proj = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=self.num_heads * self.head_dim,\n",
    "            bias=config.attention_bias,\n",
    "        )\n",
    "\n",
    "        # KV have to claim the numbers of config.num_key_value_heads \n",
    "        self.k_proj = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=self.\n",
    "            bias=config.attention_bias,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma2DecoderLayer:\n",
    "    def __init__(self, config: Gemma2Config) -> None:\n",
    "        self.self_attn = Gemma2SdpaAttention(config=config)\n",
    "        self.mlp = Gemma2MLP(config=config)\n",
    "        self.input_layernorm = Gemma2RMSNorm()\n",
    "        self.post_attention_layernorm = Gemma2RMSNorm()\n",
    "        self.pre_feedforward_layernorm = Gemma2RMSNorm()\n",
    "        self.post_feedforward_layernorm = Gemma2RMSNorm()\n",
    "\n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma2Model:\n",
    "    def __init__(self, config: Gemma2Config) -> None:\n",
    "        self.embed_tokens = torch.nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size,\n",
    "        )\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [Gemma2DecoderLayer(config=config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = Gemma2RMSNorm()\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGemma2ForCausalLM:\n",
    "    def __init__(self, config: Gemma2Config) -> None:\n",
    "        self.model = Gemma2Model(config=config)\n",
    "        self.lm_head = torch.nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
